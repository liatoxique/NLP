{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соревнование на Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                        Марк и Лия. БКЛ-151."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных конкурса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:11:54.722677Z",
     "start_time": "2018-06-20T21:11:54.718678Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "DOWNLOAD_ARGS = [\n",
    "    \"kaggle\", \"competitions\", \"download\", \"-c\", \"avito-demand-prediction\",\n",
    "    \"-f\", \"train.csv.zip\",\"-p\", \"data/\"\n",
    "]\n",
    "SUBMIT_ARGS = [\n",
    "    \"kaggle\", \"competitions\", \"submit\", \"-c\", \"avito-demand-prediction\", \"-f\",\n",
    "    \"submission.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:11:54.766663Z",
     "start_time": "2018-06-20T21:11:54.723676Z"
    }
   },
   "outputs": [],
   "source": [
    "downloader_process = subprocess.Popen(\n",
    "    DOWNLOAD_ARGS, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "if not os.path.exists(\"data/train.csv.zip\"):\n",
    "    downloader_process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распаковка и импортирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:11:57.549770Z",
     "start_time": "2018-06-20T21:11:54.799653Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:11:57.553770Z",
     "start_time": "2018-06-20T21:11:57.550770Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/train.csv\"):\n",
    "    zip_train = zipfile.ZipFile(\"data/train.csv.zip\", 'r')\n",
    "    zip_train.extractall(path='data/')\n",
    "    zip_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем одну 50ую от тренировочного файла, из-за недостаточности ресурсов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:19.366779Z",
     "start_time": "2018-06-20T21:11:57.554770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Постельные принадлежности</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кокоби(кокон для сна)</td>\n",
       "      <td>Кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Private</td>\n",
       "      <td>d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>0.12789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Другое</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Стойка для Одежды</td>\n",
       "      <td>Стойка для одежды, под вешалки. С бутика.</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>Private</td>\n",
       "      <td>79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...</td>\n",
       "      <td>692.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Видео, DVD и Blu-ray плееры</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Philips bluray</td>\n",
       "      <td>В хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>Private</td>\n",
       "      <td>b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>0.43177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02996f1dd2ea</td>\n",
       "      <td>bf5cccea572d</td>\n",
       "      <td>Татарстан</td>\n",
       "      <td>Набережные Челны</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Автомобильные кресла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автокресло</td>\n",
       "      <td>Продам кресло от0-25кг</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>286</td>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>Company</td>\n",
       "      <td>e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...</td>\n",
       "      <td>796.0</td>\n",
       "      <td>0.80323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7c90be56d2ab</td>\n",
       "      <td>ef50846afc0b</td>\n",
       "      <td>Волгоградская область</td>\n",
       "      <td>Волгоград</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили</td>\n",
       "      <td>С пробегом</td>\n",
       "      <td>ВАЗ (LADA)</td>\n",
       "      <td>2110</td>\n",
       "      <td>ВАЗ 2110, 2003</td>\n",
       "      <td>Все вопросы по телефону.</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>Private</td>\n",
       "      <td>54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>0.20797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id       user_id                 region              city  \\\n",
       "0  b912c3c6a6ad  e00f8ff2eaf9   Свердловская область      Екатеринбург   \n",
       "1  2dac0150717d  39aeb48f0017      Самарская область            Самара   \n",
       "2  ba83aefab5dc  91e2f88dd6e3     Ростовская область    Ростов-на-Дону   \n",
       "3  02996f1dd2ea  bf5cccea572d              Татарстан  Набережные Челны   \n",
       "4  7c90be56d2ab  ef50846afc0b  Волгоградская область         Волгоград   \n",
       "\n",
       "  parent_category_name               category_name  \\\n",
       "0          Личные вещи  Товары для детей и игрушки   \n",
       "1      Для дома и дачи           Мебель и интерьер   \n",
       "2  Бытовая электроника               Аудио и видео   \n",
       "3          Личные вещи  Товары для детей и игрушки   \n",
       "4            Транспорт                  Автомобили   \n",
       "\n",
       "                       param_1     param_2 param_3                  title  \\\n",
       "0    Постельные принадлежности         NaN     NaN  Кокоби(кокон для сна)   \n",
       "1                       Другое         NaN     NaN      Стойка для Одежды   \n",
       "2  Видео, DVD и Blu-ray плееры         NaN     NaN         Philips bluray   \n",
       "3         Автомобильные кресла         NaN     NaN             Автокресло   \n",
       "4                   С пробегом  ВАЗ (LADA)    2110         ВАЗ 2110, 2003   \n",
       "\n",
       "                                         description    price  \\\n",
       "0  Кокон для сна малыша,пользовались меньше месяц...    400.0   \n",
       "1          Стойка для одежды, под вешалки. С бутика.   3000.0   \n",
       "2  В хорошем состоянии, домашний кинотеатр с blu ...   4000.0   \n",
       "3                             Продам кресло от0-25кг   2200.0   \n",
       "4                           Все вопросы по телефону.  40000.0   \n",
       "\n",
       "   item_seq_number activation_date user_type  \\\n",
       "0                2      2017-03-28   Private   \n",
       "1               19      2017-03-26   Private   \n",
       "2                9      2017-03-20   Private   \n",
       "3              286      2017-03-25   Company   \n",
       "4                3      2017-03-16   Private   \n",
       "\n",
       "                                               image  image_top_1  \\\n",
       "0  d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...       1008.0   \n",
       "1  79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...        692.0   \n",
       "2  b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...       3032.0   \n",
       "3  e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...        796.0   \n",
       "4  54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...       2264.0   \n",
       "\n",
       "   deal_probability  \n",
       "0           0.12789  \n",
       "1           0.00000  \n",
       "2           0.43177  \n",
       "3           0.80323  \n",
       "4           0.20797  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df = train_df[:len(train_df)//50]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:19.371778Z",
     "start_time": "2018-06-20T21:12:19.367779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30068, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T17:13:42.476410Z",
     "start_time": "2018-06-16T17:13:40.271112Z"
    }
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:44.939584Z",
     "start_time": "2018-06-20T21:12:19.373778Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.feature_extraction\n",
    "import lightgbm as lgb\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import texterra\n",
    "import ner\n",
    "import pymorphy2\n",
    "import ufal_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем несколько моделей: модель лемматизатора (pymorphy2), модель для извлечения именованых сущностей и загружаем список стоп слов для русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:34:10.776632Z",
     "start_time": "2018-06-20T21:34:08.935221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\Anaconda3\\envs\\labs_ml\\lib\\site-packages\\ner\\extractor\\..\\model\\ner_model\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "t = texterra.API()\n",
    "syntax_model = ufal_mod.Model('russian-syntagrus-ud-2.0-170801.udpipe')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "extractor = ner.Extractor()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_ru');\n",
    "russian_stops = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программируем функционал оценивания RMSE, для будущей оценки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:49.957978Z",
     "start_time": "2018-06-20T21:12:49.954977Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    differences = predictions - targets                       \n",
    "    differences_squared = differences ** 2                   \n",
    "    mean_of_differences_squared = differences_squared.mean()  \n",
    "    rmse_val = np.sqrt(mean_of_differences_squared)         \n",
    "    return rmse_val                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим функционал в оценщик моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:49.969972Z",
     "start_time": "2018-06-20T21:12:49.959977Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_scorer = sklearn.metrics.make_scorer(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для очистки ячеек от нерусского языка, и некоторых знаков препинания; это нужно для некоторых из дальнейших морфологических обрабатывающих функционалов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:49.979968Z",
     "start_time": "2018-06-20T21:12:49.971972Z"
    }
   },
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    clean_text = re.sub(u\"[^а-яА-Я0-9.,\\-\\s]\", \" \", text)\n",
    "    clean_text = re.sub(u\"[.,\\-\\s]{3,}\", \" \", clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программируем функционал для деления на выборки (хотя в дальнейшем оказалось что это не требуется)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:49.989965Z",
     "start_time": "2018-06-20T21:12:49.980968Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        X, y, train_size=0.8)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбранная модель регрессора -- LGBM, в основном за скорость. Параметры было решено не отбирать, а оценка делалась на кросс валидации (не хотелось заниматься играми с моделями, потому что это не совсем про NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:50.001962Z",
     "start_time": "2018-06-20T21:12:49.990966Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score_lgbm(X, y):\n",
    "    estimator = lgb.LGBMRegressor(device='gpu', task='train',\n",
    "                                 boosting_type='gbdt', metric = 'rmse',\n",
    "                                 verbose=0)\n",
    "\n",
    "    score = np.mean(sklearn.model_selection.cross_val_score(estimator, X, y, scoring= rmse_scorer, cv=5))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяем выборку описаний, и целевой вектор, заменяем NA на пустые строчки, и очищаем от мусора описания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:51.069619Z",
     "start_time": "2018-06-20T21:12:50.749723Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train_df['description']\n",
    "y = train_df['deal_probability']\n",
    "X = X.fillna('')\n",
    "X = X.apply(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый векторизатор -- самый элементарный подсчитывающий, без каких-то дополнений; однако стоп слова было решено убирать сразу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T20:55:47.751806Z",
     "start_time": "2018-06-19T20:55:46.747127Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_1gram = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words=russian_stops, dtype=np.float64)\n",
    "X_1gram = vec_1gram.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T20:55:57.990541Z",
     "start_time": "2018-06-19T20:55:47.752805Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "score_1gram = get_score_lgbm(X_1gram, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\"{0:.5f}\".format(score_1gram)": "0.24759"
    }
   },
   "source": [
    "Простой векторизатор получает оценку RMSE на тестировании 0.24759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий векторизатор -- это TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:12:52.204256Z",
     "start_time": "2018-06-20T21:12:51.130601Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_1gram_tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=russian_stops)\n",
    "X_1gram_tfidf = vec_1gram_tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:13:21.071083Z",
     "start_time": "2018-06-20T21:12:52.278233Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "tfidf_1gram_score = get_score_lgbm(X_1gram_tfidf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF векторизатор улучшает оценку на  0.00011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий это 3граммы, который мы планируем присоединить к TFIDF 1 граммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:13:24.865866Z",
     "start_time": "2018-06-20T21:13:21.176053Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_3gram_tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(3, 3))\n",
    "X_3gram_tfidf = vec_3gram_tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:13:25.008820Z",
     "start_time": "2018-06-20T21:13:24.966834Z"
    }
   },
   "outputs": [],
   "source": [
    "X_13gram_tfidf = sp.sparse.hstack([X_1gram_tfidf, X_3gram_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:14:05.626838Z",
     "start_time": "2018-06-20T21:13:25.123788Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "score_13gram_tfidf = get_score_lgbm(X_13gram_tfidf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-граммы + 1-граммы TF-IDF улучшили результат по сравнению с TF-IDF 1-граммой на  0.00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем функцию, создающую последовательность POS тагов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:14:05.744802Z",
     "start_time": "2018-06-20T21:14:05.739803Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_sent_nltk(text):\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(text), lang='rus')\n",
    "    tag_sent = []\n",
    "    for word in tags:\n",
    "        tag_sent.append(word[1])\n",
    "    tag_sent = \" \".join(tag_sent)\n",
    "    return tag_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:15:40.426965Z",
     "start_time": "2018-06-20T21:14:05.859764Z"
    }
   },
   "outputs": [],
   "source": [
    "X_pos_nltk = X.apply(pos_sent_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизатор для POS был выбран обычный TFIDF монограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:15:40.961794Z",
     "start_time": "2018-06-20T21:15:40.550927Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_pos_nltk = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    sublinear_tf=True)\n",
    "X_pos_tfidf = vec_pos_nltk.fit_transform(X_pos_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:15:41.109748Z",
     "start_time": "2018-06-20T21:15:41.079757Z"
    }
   },
   "outputs": [],
   "source": [
    "X_13gram_pos_tfidf = sp.sparse.hstack([X_13gram_tfidf, X_pos_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:29.473040Z",
     "start_time": "2018-06-20T21:15:41.238707Z"
    }
   },
   "outputs": [],
   "source": [
    "score_pos_13gram_tfidf = get_score_lgbm(X_13gram_pos_tfidf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление POS улучшило результат по сравнению с TF-IDF 1-граммами + 3-граммами на 0.00016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция выделяющая именованые сущности по типу POS тэгов, то есть последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:03:30.087133Z",
     "start_time": "2018-06-19T21:03:30.084134Z"
    }
   },
   "outputs": [],
   "source": [
    "def determ_entities(sentence):\n",
    "    try:\n",
    "        entities = [ent.type for ent in extractor(sentence)]\n",
    "    except ValueError:\n",
    "        return ' '\n",
    "    return ' '.join(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:07:44.435130Z",
     "start_time": "2018-06-19T21:03:31.972529Z"
    }
   },
   "outputs": [],
   "source": [
    "X_ent = X.apply(determ_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:07:48.175933Z",
     "start_time": "2018-06-19T21:07:48.092960Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_ner = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    sublinear_tf=True)\n",
    "X_ner_tfidf = vec_pos_nltk.fit_transform(X_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:08:43.793108Z",
     "start_time": "2018-06-19T21:08:43.763117Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plus_ent = sp.sparse.hstack([X_ner_tfidf, X_13gram_pos_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:20:51.381915Z",
     "start_time": "2018-06-19T21:20:06.426758Z"
    }
   },
   "outputs": [],
   "source": [
    "score_plus_ent = get_score_lgbm(X_plus_ent, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление NER ухудшило результат по сравнению с TF-IDF 1-граммами + 3-граммами + POS на 0.00011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция лемматизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:14:22.777327Z",
     "start_time": "2018-06-19T21:14:22.773327Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lemma(sentence):\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in nltk.word_tokenize(sentence)]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:29:01.018084Z",
     "start_time": "2018-06-19T21:26:23.981751Z"
    }
   },
   "outputs": [],
   "source": [
    "X_lemma = X.apply(extract_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:29:01.972778Z",
     "start_time": "2018-06-19T21:29:01.091061Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_lemma = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    sublinear_tf=True)\n",
    "X_lemma_tfidf = vec_lemma.fit_transform(X_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:24:49.996446Z",
     "start_time": "2018-06-19T21:24:49.945463Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plus_lemma = sp.sparse.hstack([X_lemma_tfidf, X_13gram_pos_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T21:26:23.873783Z",
     "start_time": "2018-06-19T21:24:54.600971Z"
    }
   },
   "outputs": [],
   "source": [
    "score_lemma = get_score_lgbm(X_plus_lemma, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление лемматизации ухудшило результат по сравнению с TF-IDF 1-граммами + 3-граммами + POS на 0.00062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также популярным способом поднять немного сгенерировать новых признаков из этих описаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:29.609997Z",
     "start_time": "2018-06-20T21:16:29.606998Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_of_sentence(sent):\n",
    "    if sent != ' ':\n",
    "        sent_len = len(sent)\n",
    "    else:\n",
    "        sent_len = 0\n",
    "    return sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:29.743955Z",
     "start_time": "2018-06-20T21:16:29.741957Z"
    }
   },
   "outputs": [],
   "source": [
    "def voskl(sent):\n",
    "    voskl = re.compile(r\"!\")\n",
    "    voskl_count = len(voskl.findall(sent))\n",
    "    return voskl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:29.875912Z",
     "start_time": "2018-06-20T21:16:29.872913Z"
    }
   },
   "outputs": [],
   "source": [
    "def quest(sent):\n",
    "    quest = re.compile(r\"\\?\")\n",
    "    quest_count = len(quest.findall(sent))\n",
    "    return quest_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:30.013868Z",
     "start_time": "2018-06-20T21:16:30.010868Z"
    }
   },
   "outputs": [],
   "source": [
    "def three_points(sent):\n",
    "    three_points = re.compile(r\"\\.{3,}\")\n",
    "    three_points_count = len(three_points.findall(sent))\n",
    "    return three_points_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:30.144825Z",
     "start_time": "2018-06-20T21:16:30.140826Z"
    }
   },
   "outputs": [],
   "source": [
    "def determ_emotional(sent):\n",
    "    emotion = re.compile(r\"(\\?{2,})|(!{2,})\")\n",
    "    emotion_counts = len(emotion.findall(sent))\n",
    "    return emotion_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:16:30.277783Z",
     "start_time": "2018-06-20T21:16:30.273785Z"
    }
   },
   "outputs": [],
   "source": [
    "def quant_sentence(sent):\n",
    "    quant_sent = re.compile(r\"(\\.{1,})|(!{1,})|(\\?{1,})\")\n",
    "    quant_sent_count = len(quant_sent.findall(sent))\n",
    "    return quant_sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:23:27.381114Z",
     "start_time": "2018-06-20T21:23:26.541385Z"
    }
   },
   "outputs": [],
   "source": [
    "X_qs = np.asarray(X.apply(quant_sentence))\n",
    "X_de = np.asarray(X.apply(determ_emotional))\n",
    "X_tp = np.asarray(X.apply(three_points))\n",
    "X_q = np.asarray(X.apply(quest))\n",
    "X_v = np.asarray(X.apply(voskl))\n",
    "X_l = np.asarray(X.apply(len_of_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:24:49.444818Z",
     "start_time": "2018-06-20T21:24:49.442816Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_features = [X_qs, X_de, X_tp, X_q, X_v, X_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:26:17.709530Z",
     "start_time": "2018-06-20T21:26:17.704532Z"
    }
   },
   "outputs": [],
   "source": [
    "reshaped_features = []\n",
    "for feature in additional_features:\n",
    "    reshaped_features.append(feature.reshape(feature.shape[0],1))\n",
    "reshaped_features.append(X_13gram_pos_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:26:24.007512Z",
     "start_time": "2018-06-20T21:26:23.971524Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plus_eng = sp.sparse.hstack(reshaped_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T21:27:18.886547Z",
     "start_time": "2018-06-20T21:26:26.152825Z"
    }
   },
   "outputs": [],
   "source": [
    "score_eng = get_score_lgbm(X_plus_eng, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные фичи ухудшили результат на 0.00012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим извлечение синтаксиса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T01:40:07.863559Z",
     "start_time": "2018-06-21T01:40:07.727604Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_deps(text):\n",
    "    sentences = syntax_model.tokenize(text)\n",
    "    deps = []\n",
    "    for s in sentences:\n",
    "        syntax_model.tag(s)\n",
    "        syntax_model.parse(s)\n",
    "        for word in s.words:\n",
    "            deps.append(word.deprel)\n",
    "    return \" \".join(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T01:57:48.502014Z",
     "start_time": "2018-06-21T01:42:13.535833Z"
    }
   },
   "outputs": [],
   "source": [
    "X_synt = X.apply(get_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T02:02:16.619265Z",
     "start_time": "2018-06-21T02:02:15.766539Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_synt = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    sublinear_tf=True)\n",
    "X_synt_tf = vec_synt.fit_transform(X_synt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T02:02:17.404014Z",
     "start_time": "2018-06-21T02:02:17.221073Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plus_synt = sp.sparse.hstack([X_synt_tf, X_13gram_pos_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T02:04:06.625680Z",
     "start_time": "2018-06-21T02:03:03.183441Z"
    }
   },
   "outputs": [],
   "source": [
    "score_synt = get_score_lgbm(X_plus_synt, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление информации о синтаксисе ухудшило результат на 0.00016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score сабмитов на kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы, которые использовались для морфологического и синтаксического анализа достаточно тяжелые, к тому же не дали положительного или сильно положительного результата при оценке на уменьшенной выборке, поэтому сабмитов с этим типом обработки данных нет. \n",
    "\n",
    "Score с kaggle: \n",
    "- простой CountVectorizer слов - 0.2456\n",
    "- Tfidf слов - 0.2447\n",
    "- Tfidf слов и 3-грамм - 0.2446\n",
    "- Символьные n-граммы - 0.2479\n",
    "\n",
    "Некоторые примечания:\n",
    "- так как лемматизация \"ломалась\" из-за присутсвия некириллических символов, мы вычистили тексты от англоязычных слов. Однако это скорее плохое решение, т.к. среди таких слов могут оказываться названия фирм и брендов, что влияет на успешность объявления. Возможное решение: до нормализации текста вынести англоязычные слова в отдельное признаковое поле.\n",
    "- синтаксис не нужен. \n",
    "- из всех наворотов результат принесло только определение последовательностей POS_тегов\n",
    "- в задачах такого плана для создания успешного алгоритма необходимо работать со всеми данными, не только текстами объявлений (так, добавление зависимости между ценой товара и средней ценой по данной категории товаров дало небольшой прирост, но мы куда-то потеряли этот момент)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
